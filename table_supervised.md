# テーブルデータを用いた教師あり学習のアルゴリズム

**重要：万能なアルゴリズムはない！**

* 線形モデル
    * [線形回帰](#線形回帰)
    * [ロジスティック回帰](#ロジスティック回帰)
    * ※正則化
        * L1正則化
        * L2正則化
* 決定木ベースモデル
    * 決定木
    * [ランダムフォレスト](#ランダムフォレスト)
    * [勾配ブースティング三兄弟](#勾配ブースティング)
        * XGBoost：落ち着いてる長男
        * LightGBM：スピードスターの次男
        * CatBoost：個性的な三男
* 距離ベースモデル
    * k-NN
* 単純ベイズモデル
    * Naive Bayes



<a id="線形回帰"></a>
## 線形回帰

データに最もフィットする重み（w）とバイアス（b）を見つけるのが学習
損失関数： 平均二乗誤差


<a id="ロジスティック回帰"></a>
## ロジスティック回帰

データに最もフィットする重み（w）とバイアス（b）を見つけるのが学習
損失関数： 平均二乗誤差


モデルが「学習」するとは、データに最もフィットする重み（w）とバイアス（b）を見つけること



損失関数はその「フィット感」を測る指標



線形回帰の損失関数： 平均二乗誤差 (Mean Squared Error)
ロジスティック回帰の損失関数： 交差エントロピー誤差 (Cross-Entropy Error)
最小二乗誤差 → きれいなお椀（放物面）: 素晴らしいイメージです。
交差エントロピー誤差 → 人が座ったハンモック: これも良いイメージです。重要なのは、最小二乗誤差と同様に**「全体として下に凸で、ハマってしまうような変な窪み（局所解）はない」**という点です。勾配降下法で安心して底を目指せます。

$$
E = - \sum_{k=1}^{K} t_k \log y_k
$$

線形回帰の目的： 数値を予測すること（回帰問題）。
例：家の広さから家賃を予測する。


ロジスティック回帰の目的： データがどのクラスに属するかを予測すること（分類問題）。
例：勉強時間から試験に合格するか不合格になるかを予測する。


線形回帰の出力をシグモイド関数に通して変換したもの
「回帰という手法を分類という目的に利用した賢いアルゴリズム」

線形回帰：直線をフィットさせる
ロジスティック回帰：S字カーブをフィットさせる


線形回帰の場合：直線をくるくる傾けながら上下にも移動させる
ロジスティック回帰の場合：s字カーブが急になったり緩やかになったりしながら、左右にも移動させる




グラフ1：内部の線形グラフ
（横軸：特徴量x, 縦軸：線形スコアz）
これは、シグモイド関数に通す前の、線形回帰と同じ部分のグラフです。z = wx + b で表されるただの直線です。
グラフ2：最終的なS字カーブ
（横軸：特徴量x, 縦軸：確率p）


線形グラフをくるくる回すと、s字グラフも連動してビヨンビヨン急峻さが変わる。右上がりか右下がりか変わるたびにs字グラフもパッと反転し、線形グラフと同じく右上がりか右下がりになる
線形グラフを上下に動かすと、グラフと横軸が交わる点が左右に動く。ここが 決定境界を表しているため、連動してs字グラフは左右に動く。


あなたの持っている動的なイメージは、ロジスティック回帰の学習プロセスそのものです。
傾き w を調整して → S字カーブの形（急峻さ）と向きをデータに合わせる。
切片 b を調整して → S字カーブの**位置（決定境界）**をデータに合わせる。
この2つの操作を繰り返して、データ（クラス0の点とクラス1の点）を最も上手く分離できるS字カーブを探し出すのが、ロジスティック回帰の学習アルゴリズムです。






勾配ベクトルの始点は、今自分が立っている山の斜面から垂直方向に真下にある底面の点
その点から「斜面がこっち方向に一番上ってますよ!」という情報を表す一本の矢印が伸びている

「今自分が立っている山の斜面」というのが、(n+1)次元空間上の点 (w1, w2, ..., wn, Loss) です。
「垂直方向に真下にある底面の点」というのが、n次元のパラメータ空間上の点 (w1, w2, ..., wn) です。
この点が、勾配ベクトルという矢印の**始点（出発点）**になります。まさにその通りです。



まとめると
あなたが今立っている場所（パラメータ）から、損失という「山」の最も低い場所（最適解）を探したいとき、
1. まず、自分が立っている山の斜面の真下、地図（パラメータ空間）上の現在地を確認します。（＝ベクトルの始点）
1. 次に、その場所で最も急な上り坂がどちらを向いているかを示す**コンパス（勾配ベクトル）**を見ます。
1. そして、そのコンパスが指し示す方向とは真逆の方向に一歩踏み出します。（＝勾配降下法）
この1〜3のプロセスを繰り返していくのが、勾配降下法による最適化です。


目的: 損失関数の最小化（山下り）
地図: パラメータ空間
地形: 損失関数のグラフ（山や谷）
道具: 微分（コンパス・傾斜計）
戦略: 勾配降下法（道具を使って、常に一番急な下り坂を進む）



パラメータ空間に、勾配ベクトルというガイド情報を投影する。
「損失関数から計算された勾配という『ガイド情報』を、パラメータ空間に『投影』し、その指示に従って現在のパラメータ（座標）を移動させる。その移動の勢いを調整するのが学習率である。」
?学習率 η は、その「ナビ情報」を、実際にFを動かすための**「具体的な修正量」に変換する**ための、非常に重要な”翻訳係”の役割も担っているのです。

勾配下降法について
以下の式は正しい？
newパラメータベクトル　= oldパラメータベクトル + 学習率 * (-勾配ベクトル)

学習率で「勾配」を「パラメータの更新量」に変換する

### 交差エントロピー誤差（対数損失）とは？

そこで登場するのが**交差エントロピー誤差**です。この損失関数は、最尤推定（MLE）の考え方から導出され、ロジスティック回帰の学習において非常に都合の良い性質を持っています。

#### 1. 考え方

「モデルが予測した確率分布」が、「実際の正解ラベルの確率分布」にどれだけ近いかを測る指標近ければ近いほど損失は小さくなります。

データ1件あたりの損失関数

$$ L_{CE}(p, y) = - [ y \log(p) + (1-y) \log(1-p) ] $$

*   $y=1$ のとき、第2項は消えて $- \log(p)$ となる。
*   $y=0$ のとき、第1項は消えて $- \log(1-p)$ となる。



すべての訓練データ（$m$件）に対する全体の損失関数は、これらの平均を取ったものになる。  

$$ J(\boldsymbol{w}, b) = - \frac{1}{m} \sum_{i=1}^{m} [ y_i \log(p_i) + (1-y_i) \log(1-p_i) ] $$
※$p_i = \sigma(\boldsymbol{w}^T \boldsymbol{x}_i + b)$


### 交差エントロピー誤差を使うメリット

1.  **凸関数になる**:
    この損失関数 $J(\boldsymbol{w}, b)$ は、パラメータに対して**凸関数**になります。これにより、勾配降下法を使えば、必ず唯一の最適解（損失が最小になる点）に収束することが保証されます。これが最大の利点です。

2.  **確率的な解釈（最尤推定）**:
    交差エントロピーを最小化することは、ベルヌーイ分布を仮定した**最尤推定を行うことと等価**です。これにより、統計的に非常に妥当な方法でパラメータを推定していることになります。

3.  **勾配の計算がシンプル**:
    この損失関数をパラメータで偏微分すると、非常にきれいな形になります。
    $$ \frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} (p_i - y_i) x_{ij} $$
    この「予測と正解の誤差」に比例した形で重みを更新するという直感的で美しい式が得られ、効率的に学習を進めることができます。

---

### まとめ

| | 線形回帰 | ロジスティック回帰 |
|:---:|:---:|:---:|
| **モデルの出力** | 連続値 | 確率 (0〜1) |
| **デファクトスタンダード<br>の損失関数** | **最小二乗誤差 (MSE)** | **交差エントロピー誤差 (Log Loss)** |
| **主な理由** | 数学的に扱いやすく、誤差が正規分布に従うという仮定のもとで最尤推定と一致する。 | 損失関数が**凸関数**になり、安定した学習が可能。ベルヌーイ分布の仮定のもとで最尤推定と一致する。 |

結論として、ロジスティック回帰の損失関数は**交差エントロピー誤差**がデファクトスタンダードであり、理論的にも実践的にも最も優れた選択肢とされています。



、ロジスティック回帰の交差エントロピー損失の場合、この曲面は**下に凸の、きれいな「お椀型」**になります。









































<a id="ランダムフォレスト"></a>
## ランダムフォレスト

* 沢山の小さな決定木をつくり組み合わせる  
* データと特徴量をランダムにサンプリングし、多様な木を作る
* 多数決や平均をとることで、個々の木の予測の不安定さを打ち消し、安定した予測を得る。  


<a id="勾配ブースティング"></a>
## 勾配ブースティング

* 沢山の小さな決定木をつくり組み合わせる  
* 新しいモデルは、前のモデルの予測と実際の値との残差(損失関数の勾配)を予測するように学習  
* 各モデルの結果を足し合わせる  

[ゼロから始める勾配ブースティング決定木の理論](https://zenn.dev/dalab/articles/9c843f0ec8aabf)









