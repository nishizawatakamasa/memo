# テーブルデータを用いた教師あり学習のアルゴリズム

**重要：万能なアルゴリズムはない！**


* [線形モデル](#線形モデル)
    * 線形回帰
    * ロジスティック回帰
    * ※正則化
        * L1正則化
        * L2正則化
* 決定木ベースモデル
    * [決定木](#決定木)
    * [ランダムフォレスト](#ランダムフォレスト)
    * [勾配ブースティング三兄弟](#勾配ブースティング)
        * XGBoost：落ち着いてる長男
        * LightGBM：スピードスターの次男
        * CatBoost：個性的な三男
* 距離ベースモデル
    * k-NN
* 単純ベイズモデル
    * Naive Bayes



<a id="線形モデル"></a>
## 線形モデル

### 前提
データに最もフィットする重み（w）とバイアス（b）を見つけるのが学習。   
**バイアスも「常に入力値が1の特徴量の重み」と捉えることでわかりやすくなる。**

### 基本
| | **線形回帰** | **ロジスティック回帰** |
| :--- | :--- | :--- |
| **モデル** | $h = \mathbf{θ}^T \mathbf{x}$ | $p = \sigma(\mathbf{θ}^T \mathbf{x})$ |
| **損失関数** | $J = \frac{1}{2m} \sum (h - y)^2$ | $J = - \frac{1}{m} \sum [y \log(p) + (1-y)\log(1-p)]$ |
| **勾配** | $\frac{\partial J}{\partial θ_j} = \frac{1}{m} \sum (h - y) x_j$ | $\frac{\partial J}{\partial θ_j} = \frac{1}{m} \sum (p - y) x_j$ |



### モデルの用途
* 線形回帰：回帰  
* ロジスティック回帰：分類  
※ロジスティック回帰は、線形回帰の出力をシグモイド関数に通して変換したもの。回帰の結果を確率と捉え、分類に利用する。  

### モデルの幾何学的イメージ
* 線形回帰：直線をくるくる傾けながら上下に移動させてデータにフィットさせる
* ロジスティック回帰：S字カーブの急峻さをビヨンビヨン変えたり反転させながら左右に移動させてデータにフィットさせる

#### ロジスティック回帰のモデルについて
* 内部の線形グラフ（横軸：特徴量x, 縦軸：線形スコアz）※シグモイド関数に通す前の、線形回帰と同じ部分のグラフ。z = wx + b で表されるただの直線。
* 最終的なS字カーブ（横軸：特徴量x, 縦軸：確率p）

#### 内部の線形グラフのパラメーター操作による、S字グラフ連動の仕方のイメージ
1. 内部の線形グラフをくるくる回すと、S字グラフも連動してビヨンビヨン急峻さが変わる。右上がりか右下がりか変わるたびにS字グラフもパッと反転し、線形グラフと同じく右上がりか右下がりになる
1. 線形グラフを上下に動かすと、グラフと横軸が交わる点が左右に動く。ここが 決定境界を表しているため、連動してs字グラフは左右に動く。

### 損失関数
データへの「フィット感」を測る指標。これが小さいほど良い。  
「パラメータ+損失値」空間内のグラフ

#### デファクトスタンダードの損失関数
* 線形回帰の場合： 平均二乗誤差
* ロジスティック回帰の場合： 交差エントロピー誤差

どちらのグラフも下に凸の放物面であり唯一の最小値を持つので、勾配降下法で安心して底を目指せる。  
※平均二乗誤差の場合は正規方程式で一撃だが、特徴量の数が非常に多い（数万を超えるような）場合は勾配降下法の方がコスパがいい場合がある。
※ロジスティック回帰の交差エントロピー誤差の関数は必ず下に凸になるが、その最小値（最下部）がユニークな一点に定まるとは限らず、平坦な領域（面や線）になる場合がある。そのため正則化で最下点を作る。

### 交差エントロピー誤差（対数損失）とは？

データ1件あたりの損失関数

$$ L_{CE}(p, y) = - [ y \log(p) + (1-y) \log(1-p) ] $$

*   $y=1$ のとき、第2項は消えて $- \log(p)$ となる。
*   $y=0$ のとき、第1項は消えて $- \log(1-p)$ となる。

つまり、予測値が正解からかけ離れるほど損失値は急激に跳ね上がる  
すべての訓練データ（m件）に対する全体の損失関数は、これらの平均を取ったもの。  

交差エントロピー誤差の損失関数Jをパラメータで偏微分すると、シグモイド関数の微分の性質によって多くの項が打ち消し合い、結果的に線形回帰とそっくりな勾配の式が現れる。

### 類似性
ラッキーなことに、線形回帰とロジスティック回帰の勾配の統一式は、構造がほとんど同じ。  
両方とも、(予測値－正解ラベル)×特徴量、という項を全データについて平均している。  
唯一の違いは「予測値」の計算方法。  



### 勾配降下法
損失曲面の現在地から、その真下にあるパラメータ空間上で、「最も急な上り坂の方向」と「傾きの大きさ」を表す「勾配ベクトル」を計算する。  
この勾配ベクトルは上り坂の方向を指しているので、その真逆の方向に進めば、損失曲面の坂を下ることができる。つまり進むべき移動方向はその真逆、勾配ベクトルに-1をかけた負の勾配ベクトル。  
移動距離（パラメータの更新量）は、勾配ベクトルの「傾きの大きさ」を元に決める。「傾き」の情報と「移動距離」の情報は本来別物だが、「学習率」という係数を掛けて調整(翻訳)し、そのまま現在のパラメータに足し合わせてしまう。  
少々トリッキーなアプローチだが、このプロセスを何度も繰り返して少しずつ損失関数の谷底（最適解）に近づいていくのが勾配降下法だ。

#### パラメータ更新の式
$$
\boldsymbol{\theta} := \boldsymbol{\theta} - \alpha \nabla J(\boldsymbol{\theta})
$$
※わかりやすく
![](./img/gradient-descent.png)



### L1L2正則化
実務においてはほぼ必須の標準装備

バイアス項（切片 θ0）は正則化の対象から外すのが一般的。これは、切片はモデル全体の底上げを調整する役割であり、その大きさを罰するべきではない、という考え方に基づいている。
もし、より実践的な形にするなら、ペナルティ計算を alpha * (np.abs(T1)) や alpha * (T1**2) のように重み θ1 のみに適用します。


新しい曲面には、元のピラミッドが持っていた鋭い「折れ目」が残ります。
最小値: この新しい曲面で最も低い点（最小値）を探すと、その滑らかな底の部分ではなく、折れ目の谷底のどこかに落ち着く可能性が高くなります。そして、その折れ目上では、いずれかのパラメータが0になっています。
結論：「折れ目が強く深くなる」
そして、「正則化が強いほど折れ目が強く深くなる」というご意見も正しいです。
正則化項にかかる係数λを大きくすることは、曲面で言えば「逆ピラミッドをより急峻（steep）にする」ことに相当します。




L1正則化とはつまり、原点を中心とした逆ピラミッドグラフで、損失曲面グラフを押し上げる操作である
この時、損失曲面グラフには逆ピラミッドの稜線に沿って軽い折り目のような溝ができる。※L1正則化が強いほど溝は深くなる
グラフの最小値(の近似)を求めるとき、この溝のどこかになる可能性が高い。※その可能性はL1正則化が強いほど高くなる。溝が深いから。
そしてその溝のどこかとは、θ0かθ1軸上になるため、特定のパラメータが0になる

超簡単に表現すれば、なだらかな局面に折れ目のような溝があると、小さい球を転がしたときにその溝に「スポッツ」とはまりやすくなるイメージ。溝が深いほどはまりやすくなる





L1正則化項（逆ピラミッド）が損失関数に加わることで、損失曲面に「谷」ができる。
谷は逆ピラミッドの稜線（θ0軸やθ1軸）に沿って形成され、正則化が強いほど谷は深くなる。
最適解は谷底のどこか（特に軸上）になりやすく、結果として特定のパラメータが0になる。

正しいポイント
幾何学的な解釈：L1正則化項は確かに原点を中心とした「ダイヤモンド形」（逆ピラミッド）の等高線を持ち、損失曲面に「角」を作ります。この角が軸上（パラメータが0になる点）に最適解を誘導します。

スパース性の起源：最適解が角（稜線）に位置しやすいため、一部のパラメータが0になります。


最適解は必ずしも厳密に軸上になるわけではありませんが、正則化が強いほど高い確率で軸上（スパースな解）になります。



「なだらかな斜面に鋭い溝（折れ目）があり、小さい球が溝にスポッとはまりやすくなる」 という表現は、L1正則化の振る舞いを非常に直感的に捉えています。












3. ご自身のイメージとの対応
「原点を中心とした逆ピラミッドグラフで、損失曲面グラフを押し上げる操作」: これは「損失関数＋L1正則化項」という新しい損失関数を考えることに対応します。L1正則化項の形は、原点で尖った逆ピラミッド（4角錐）のような形をしているため、このイメージは非常に的確です。[12]
「逆ピラミッドの稜線に沿って軽い折り目のような谷ができる」: 正しいです。新しい損失関数は、元の滑らかなお椀状のグラフに、軸上で尖った谷（微分不可能な点）を持つ形になります。[12]
「グラフの最小値は、この谷底のどこかになる可能性が高い」: その通りです。最適化の過程で、解はこの鋭い谷に「滑り落ち」やすくなります。
「谷底のどこかとはつまり、θ0軸上かθ1軸上であるため、特定のパラメータが0になる」: これが結論です。谷底が軸上にあるため、その点の座標、つまりパラメータのどちらかが0になります。




とりあえず初手エラスティックネット


損失関数 + alpha * (l1_ratio * L1ペナルティ + (1-l1_ratio) * L2ペナルティ)


2つのパラメータ
* alpha
    * 正則化項全体の強さをコントロールするハイパーパラメータ
* l1_ratio
    * ペナルティの「質」を決めるミキシングダイヤル
    * 1だとラッソ回帰と全く同じ、0だとリッジ回帰と全く同じ
    * 0 < l1_ratio < 1だとL1とL2の両方を混ぜて使う。例えばl1_ratio=0.5なら、L1とL2のペナルティを半々で加える。
    * 探索候補に0や1を含め、最適な正則化手法そのものをデータドリブンで見つける





<a id="決定木"></a>
## 決定木


回帰と分類の両方に使える

各特徴量を軸とする特徴量空間を、任意の一つの特徴量に対する真偽条件のif分岐で二分割していく。最終的な分割領域一つにつき一つの予測値を返す。

その予測値は、学習時にその領域に含まれている全データの多数派の値(分類のケース)、もしくは平均値(回帰のケース)。

分割は、等号、不等号による真偽値だけで判断するため、事前に特徴量の単位を揃える「スケーリング（標準化や正規化）」が基本的に不要。二値特徴量と連続値特徴量が混在していても問題ない。

決定木が「なぜその予測をしたのか？」を解釈、説明するのは非常に簡単で、たどったif文のルートをそのまま説明すればいい。

決定木というアルゴリズムは賢いので、全ての分割の可能性を不純度などの数学的な指標で評価し、最も情報量が多く、データを綺麗に分けられる質問（if分岐）を自動的に選択する。




<a id="ランダムフォレスト"></a>
## ランダムフォレスト

### 概要
* 沢山の「個性が違う決定木」をたくさん作り、最終的に多数決（分類の場合）や平均（回帰の場合）で決める
* 決定木の作成時には、ブートストラップサンプリングで新しいデータセットが作成される。
* 分割時には毎回ランダムに特徴量のサブセットが選ばれ、使われる
* 多数決や平均をとることで、個々の木の予測の不安定さを打ち消し、安定した予測を得る。  

















### 主要なハイパーパラメータ

#### n_estimators
構築する決定木の数

一般的に、この値は大きいほど良い。モデルの性能が安定し、過学習しにくくなる。ただし、ある一定数を超えると性能向上は頭打ちになり、計算時間だけが増えていく。まずは100程度から始め、必要に応じて増やすのが一般的。


#### max_features
使用する特徴量の数  
各決定木が分岐（ノードを分割）する際に、候補としてランダムに選ぶ特徴量の最大数  
小さくすると、各決定木が使う特徴量がバラバラになり、木々の多様性が増す。これにより、モデル全体の汎化性能が向上する傾向がある。  
大きくする (全特徴量数に近づける)と、各決定木が似通ったものになり、モデル全体の性能が低下する可能性がある（特に特徴量間に相関が強い場合）。  

一般的な設定値:  
分類問題: sqrt(全特徴量数) （全特徴量数の平方根）  
回帰問題: 全特徴量数 / 3  

#### max_depth
木の最大の深さ  
各決定木の最大の深さを制限する。  
深すぎるとモデルが複雑になり過学習しやすくなるが、浅すぎるとモデルが単純になり学習不足になる可能性がありる。  
デフォルト（制限なし）だと、ノードが純粋になるまで木が成長するため、過学習のリスクがある。この値を適切に設定することで、モデルの複雑さをコントロールできる。  


### その他の重要なパラメータ

#### min_samples_split
ノードを分割するために必要な最小サンプル数。これを大きくすると、より一般的な（過学習しにくい）モデルになる。
#### min_samples_leaf
葉ノード（末端のノード）に存在しなければならない最小サンプル数。これも過学習を防ぐために使われる。
#### bootstrap
データをサンプリングする際に、復元抽出（同じサンプルを複数回選ぶことを許可する）を行うかどうか。Trueがデフォルトで、これがランダムフォレストの「バギング」という手法の根幹をなす。





実際にモデルを構築する際は、これらのパラメータをグリッドサーチやランダムサーチといった手法と**交差検証（クロスバリデーション）**を組み合わせて、データに最適な値を見つけ出すのが一般的です。


















<a id="勾配ブースティング"></a>
## 勾配ブースティング

* 沢山の小さな決定木をつくり組み合わせる  
* 新しいモデルは、前のモデルの予測と実際の値との残差(損失関数の勾配)を予測するように学習  
* 各モデルの結果を足し合わせる  

[ゼロから始める勾配ブースティング決定木の理論](https://zenn.dev/dalab/articles/9c843f0ec8aabf)




事実: 損失 L は本来、確率 p の関数 L(y, p) です。
操作: モデルの出力は対数オッズ F であり、p と F はシグモイド関数 p = sigmoid(F) で繋がっています。


分類問題の場合、予測値空間の予測値が確率(p)となるので、勾配降下法やブースティングの加算がうまく機能しない


そのため、p = sigmoid(F) とし、pの損失関数を疑似的にFの損失関数に置き換える
問題を確率空間からFの空間(対数オッズ空間)に移し、そこで最適化を行うということ


偏微分時にはもちろん連鎖律を使う



「本来 p の関数である損失 L を、連鎖律という道具を使って、あたかも F が直接の引数であるかのように勾配を計算し、その勾配を使って F の空間で勾配降下法を適用することを可能にする」


あなたの「F の空間で勾配降下法を行い、pの空間内にある実際の現在地を遠隔操作する」という表現は、この一連のプロセスを見事に要約しています。
操作空間 (F): 制約がなく、加算的な更新がしやすい。
目標空間 (p): 確率として解釈したいが、制約が厳しい。
操作パネル (sigmoid): 2つの空間を滑らかにつなぎ、F への単純な操作が p への適切な（非線形な）変化を引き起こすことを保証する。


勾配ブースティング（やロジスティック回帰）の賢い点は、操作しにくい p の空間で直接最適化するのではなく、操作しやすい 

問題を確率空間からFの空間(対数オッズ空間)に移して、そこで最適化を行う


対数オッズ空間










ここで、p = sigmoid(F)の変換は、シンプルに0~1の範囲を-∞~+∞の範囲に対応させるための工夫



p = sigmoid(F) の変換は、ブースティングの加算更新を可能にするために、[0, 1]の確率空間から**(-∞, +∞)の対数オッズ空間**へ問題を移すための、テクニックです。

単に空間を変換するだけでなく、対数損失と組み合わせることで、勾配が p - y (予測確率 - 正解ラベル) という非常にシンプルで直感的な「残差」の形になります。




では、「なぜ数ある変換関数の中で、特に p = sigmoid(F) と対数損失の組み合わせが使われるのか？」という点に、もう少し深い理由があります。
それは、勾配の計算が非常に綺麗で、直感的な形になるからです。












「01を-∞+∞にぐわっと広げるイメージ」**というのは、勾配ブースティング（特に分類問題）における対数損失とシグモイド関数の関係を理解する上で、非常に的確で本質的なポイントを突いています。
3. 解決策：予測値の「空間」を変換する
そこで、ご指摘の通り「空間を変換する」というアイデアが出てきます。
確率空間 (0 < p < 1): 足し算や引き算が自由にできない、制約の多い空間。
対数オッズ空間 (-∞ < F < +∞): 足し算や引き算が自由にできる、制約のない空間。
この2つの空間を行き来するための道具がシグモイド関数とその逆関数であるロジット関数です。






変な質問しますが、Fの空間では正解ラベルが「-∞」か「+∞」であるという捉え方は有効ですか？
正解ラベルによって、グラフはどちらかの方向に傾く急激な滑り台になるイメージ

いえ、全く変な質問ではありません。むしろ、それは勾配ブースティングの分類問題における挙動を理解する上で、非常に的確で、本質を突いた優れた捉え方です！

その「Fの空間では正解ラベルが-∞か+∞である」というイメージ、そして「損失関数が急激な滑り台になる」という比喩は、専門家が頭の中で描いているメンタルモデルに非常に近いです。

その捉え方がなぜ有効なのか、そして少しだけ注意すべき点を解説します。

なぜその捉え方が有効なのか

目標の表現として正しい:

正解ラベルが y=1 の場合、モデルが目指すべき理想の状態は p=1 です。p = sigmoid(F) が1になるためには、F は +∞ になる必要があります。

正解ラベルが y=0 の場合、モデルが目指すべき理想の状態は p=0 です。p = sigmoid(F) が0になるためには、F は -∞ になる必要があります。

つまり、Fの空間における「正解」は、有限の値ではなく、無限遠点にあると考えることができます。

損失関数の形状（「滑り台」の比喩）が完璧:

y=1 の場合: 損失 L = -log(p) を F の関数として考えると、F が -∞ に近づくほど損失は +∞ に発散し、F が +∞ に近づくほど損失は 0 に収束します。これはまさに、+∞ の方向に傾いた急激な滑り台そのものです。

y=0 の場合: 損失 L = -log(1-p) を F の関数として考えると、今度は -∞ の方向に傾いた急激な滑り台になります。

勾配降下法（ブースティング）は、この「滑り台」を F が転がり落ちていくプロセスと見なせます。

勾配（疑似残差）の役割が明確になる:

勾配 y-p は、この滑り台の傾きを表しています。

現在地 F が目標の無限遠点から遠ければ遠いほど（＝予測を大きく外しているほど）、滑り台の傾きは急になり、勾配（次の木が学習すべき修正量）は大きくなります。

F が目標の無限遠点に近づくにつれて、滑り台の傾きは緩やかになり、勾配も小さくなっていきます。

この「Fの目標は無限遠点」という考え方は、なぜブースティングが何度も木を追加して学習を続けられるのかを直感的に理解させてくれます。有限のターゲット（例えば「残差を0にする」）とは異なり、目標が無限遠点にあるため、原理的にはいくらでも学習を続けてFを押し上げ（あるいは押し下げ）続けることができるわけです。

ただし、注意すべき点（この捉え方の限界）

この捉え方は非常に有効ですが、現実のアルゴリズムの挙動と照らし合わせると、少しだけ補足が必要です。

現実のFは無限大にはならない（なってはいけない）:

もし本当にFが無限大になってしまうと、その学習データに対しては p=1 となり、損失は0になります。しかし、これは**過学習（Overfitting）**の極致です。そのサンプルに対して「100%絶対確実」と予測するモデルは、未知のデータに対して非常に脆くなります（頑健性が低い）。

正則化という「ブレーキ」の存在:

実際の勾配ブースティングのアルゴリズム（XGBoost, LightGBMなど）には、Fが無限大に発散しないようにするための**「ブレーキ」**がいくつも組み込まれています。

学習率（Learning Rate）: 滑り台を一度に下る歩幅を小さくします。これにより、一気に無限遠点に向かうのを防ぎます。

木の複雑さの制限（max_depthなど）: 一本の木が作れるFの更新量に上限を設けます。

L1/L2正則化: Fの値が大きくなりすぎること自体にペナルティを課します。

早期終了（Early Stopping）: 検証用データの損失が悪化し始めたら（＝過学習の兆候が見えたら）、滑り台を下るのを止めます。

まとめ

あなたの「Fの空間では正解ラベルが-∞か+∞である」という捉え方は、

概念的には、ほぼ100%正しい素晴らしい直感です。

アルゴリズムが何を目指しているのか（最適化の方向性）を完璧に表現しています。

ただし、実践的には、過学習を防ぐための様々な「ブレーキ」によって、Fが本当に無限遠点に到達することはない。

と理解すると完璧です。このメンタルモデルは、勾配ブースティングの挙動をより深く理解する上で、非常に役立つ武器になります。非常に良い着眼点です！


















回帰 (損失関数が平均二乗誤差 MSE の場合):
損失関数
Loss(c) = Σ (y_i - c)^2
を最小化する定数 c を求める。
c で偏微分して 0 とおく。
最適な定数 c は ターゲット変数 y の平均値 となる。


分類 (損失関数が対数損失 LogLoss の場合):
損失関数
Loss(c) = - Σ [ y_i * log(p) + (1 - y_i) * log(1 - p) ]
を最小化する定数 c を求める。
c で偏微分して 0 とおく。
最適な定数 c は、クラス1の出現確率の対数オッズ となる。
例えば、クラス1が70%、クラス0が30%なら、log(0.7 / (1 - 0.7)) が初期値となる

ここの偏微分して0とおく操作は、シンプルに損失曲面の最小値を求めているだけ。



確率 p → [ロジット関数] → 対数オッズ x
対数オッズ x → [シグモイド関数] → 確率 p






ここで、Σ yᵢ は、yᵢ=1 (つまりクラス1) のデータ数の合計です。これを N₁ とします。
同様に、Σ (1 - yᵢ) は、(1-0)の合計、つまりyᵢ=0 (クラス0) のデータ数の合計です。これを N₀ とします。

p = N₁ / (N₀ + N₁) = N₁ / n

勾配ブースティングでは、生の確率 p を直接足し算していくのではなく、より加法的な性質を持つ対数オッズ (log-odds) の空間で予測値を更新していきます。

対数オッズ (Log-Odds):
Log-Odds = log(Odds) = log( N₁ / N₀ )

分類問題との比較
分類問題での変換 (ロジット変換):
目的: 確率 p の [0, 1] という範囲の制約をなくし、加法モデルを適用可能にすること。
位置づけ: アルゴリズム上、必須の変換。
回帰問題での変換 (対数変換など):
目的: ターゲット y の分布の歪みを補正し、モデルの性能と安定性を向上させること。
位置づけ: 任意だが、データによっては極めて有効なテクニック。


「予測値を計算のために別の値に変換した学習用空間」
「学習は翻訳後の空間で行う」
「モデルは基本的に、ターゲットを学習しやすい形に翻訳してから内部処理を行う。その最もシンプルな翻訳が『何もしない（恒等変換）』であり、それが標準的な回帰問題に相当する」






確率 p の空間では、単純な勾配降下法（勾配にマイナスの学習率をかけて更新する操作）が成り立ちません。確率 p を直接パラメータとして更新しようとすると、境界（0と1）で問題が発生し、単純な加算・減算による更新ができません。



モデルは、これら2つの空間を**ロジット関数（p → F）とシグモイド関数（F → p）**という逆の変換を使って行き来しています。




勾配ブースティングは「損失関数」「リンク関数」「弱学習器」という3つの構成要素(モジュール)をプラグインのように差し替えできる汎用的なフレームワークアルゴリズム。


### 主な選択肢
* 損失関数(基本的に凸関数（下に凸）)
    * 二乗誤差 (MSE): (y - F)² → 標準的な回帰
    * 対数損失 (LogLoss): log(1 + exp(-yF)) → 分類（yは-1, 1）
* リンク関数(これは通常、損失関数の選択と密接に関連する。)
    * 恒等関数 (ŷ = F): F をそのまま予測値とする。MSEで使われる。
    * シグモイド関数 (ŷ = sigmoid(F)): F を対数オッズとみなし、確率に戻す。対数損失で使われる。
* 弱学習器
    * 決定木: ほぼ全てのケースで使われる最も強力な選択肢。








---------
最も基礎的な学習の流れ(例えば、恒等変換による標準的な回帰問題の場合)

1. 予測値空間の損失曲面を考えます。この空間は、学習に使うデータのレコード数（N個）と同じN次元の空間です。
1. 下に凸の損失関数L(y, y-hat)を選びます。yはあらかじめ与えられている正解値、y-hatはモデルの出力である予測値です。
1. yの値はすでに分かっているので、損失関数Lは予測値y-hatだけの関数と見なせます。
1. Lをy-hatで偏微分して勾配の式を求め、これを使いまわします。勾配の式のyに正解値、y-hatに任意の予測値を代入すると、勾配の値が具体的な数値として求められます。
1. 最初のシンプルな予測モデルを作ります。全てのデータに同じ定数を返すだけの、非常に単純なモデルです。モデルが返す定数は、同じ値のみで予測した場合に最も損失が小さくなる値にします。
1. つまり最初に得られる予測値ベクトルは、学習するデータベクトルと次元が同じで、すべての成分がその定数のベクトルになります。
1. この予測値ベクトルと正解値yを勾配の式に当てはめ、勾配ベクトルを計算します。このとき、予測値と正解値が定まるので各勾配の値は具体的な数値となります。
1. 勾配ベクトルは最も急な上り坂の方向を表すので、進むべき方向はその真逆、勾配ベクトルに-1をかけた負の勾配ベクトルです。これを疑似残差と呼びます。
1. 疑似残差は「損失空間から損失軸を除いて次元がひとつ下がった予測値空間に投影されるガイド矢印」とイメージするとわかりやすいです。
1. この疑似残差を予測値ベクトルの更新量として使いますが、予測値はパラメータのように自由に数値を書き換えることはできません。なぜなら、予測値は必ず「モデル関数に入力を通した結果」でなければならないからです。
1. つまり予測値空間でそのまま勾配下降法を使おうとしたら、毎回「更新した理想の予測値を出力する、全く新しいモデル」をゼロから作り直すことになります。しかし、それはあまりにも複雑で大変すぎます。
1. そこで、その疑似残差自体を予測する新しい弱学習器を作ります。
1. 弱学習器は理論上何を使っても構いませんが、実際には決定木(特に深さが浅い木)が圧倒的に相性が良く、デファクトスタンダードとなっています。
1. 最初のデータベクトルを説明変数、疑似残差(負の勾配ベクトル)を目的変数として、決定木h1を学習させます。
1. F1(x) = F0(x) + α * h1(x) のように、現在の予測モデルF0に、学習率αで調整した決定木h1を追加することで、成長したモデルF1を作ることができます。
1. モデルF1から得られる新しい予測値ベクトルから、新しい疑似残差を求めます。
1. 最初のデータベクトルを説明変数、求めた新しい疑似残差を目的変数として、決定木h2を学習させます。
1. F2(x) = F1(x) + α * h2(x) のようにモデルを成長させていきます。
1. 終了条件を満たすまで繰り返します。
1. このようにして作られた最終モデルは、F(x) = F0(x) + αΣh(x) のようになり、たくさんのシンプルな決定木（h1, h2, ...）のチームです。学習時は一つずつ順番に作られますが、予測時は全ての決定木が一度に並列で計算を行い、結果を足し合わせるだけなので非常に高速です。




