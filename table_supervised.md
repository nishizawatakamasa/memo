# テーブルデータを用いた教師あり学習のアルゴリズム

**重要：万能なアルゴリズムはない！**


* [線形モデル](#線形モデル)
    * 線形回帰
    * ロジスティック回帰
    * ※正則化
        * L1正則化
        * L2正則化
* 決定木ベースモデル
    * [決定木](#決定木)
    * [ランダムフォレスト](#ランダムフォレスト)
    * [勾配ブースティング三兄弟](#勾配ブースティング)
        * XGBoost：落ち着いてる長男
        * LightGBM：スピードスターの次男
        * CatBoost：個性的な三男
* 距離ベースモデル
    * k-NN
* 単純ベイズモデル
    * Naive Bayes



<a id="線形モデル"></a>
## 線形モデル

### 前提
データに最もフィットする重み（w）とバイアス（b）を見つけるのが学習。   
**バイアスも「常に入力値が1の特徴量の重み」と捉えることでわかりやすくなる。**

### 基本
| | **線形回帰** | **ロジスティック回帰** |
| :--- | :--- | :--- |
| **モデル** | $h = \mathbf{θ}^T \mathbf{x}$ | $p = \sigma(\mathbf{θ}^T \mathbf{x})$ |
| **損失関数** | $J = \frac{1}{2m} \sum (h - y)^2$ | $J = - \frac{1}{m} \sum [y \log(p) + (1-y)\log(1-p)]$ |
| **勾配** | $\frac{\partial J}{\partial θ_j} = \frac{1}{m} \sum (h - y) x_j$ | $\frac{\partial J}{\partial θ_j} = \frac{1}{m} \sum (p - y) x_j$ |



### モデルの用途
* 線形回帰：回帰  
* ロジスティック回帰：分類  
※ロジスティック回帰は、線形回帰の出力をシグモイド関数に通して変換したもの。回帰の結果を確率と捉え、分類に利用する。  

### モデルの幾何学的イメージ
* 線形回帰：直線をくるくる傾けながら上下に移動させてデータにフィットさせる
* ロジスティック回帰：S字カーブの急峻さをビヨンビヨン変えたり反転させながら左右に移動させてデータにフィットさせる

#### ロジスティック回帰のモデルについて
* 内部の線形グラフ（横軸：特徴量x, 縦軸：線形スコアz）※シグモイド関数に通す前の、線形回帰と同じ部分のグラフ。z = wx + b で表されるただの直線。
* 最終的なS字カーブ（横軸：特徴量x, 縦軸：確率p）

#### 内部の線形グラフのパラメーター操作による、S字グラフ連動の仕方のイメージ
1. 内部の線形グラフをくるくる回すと、S字グラフも連動してビヨンビヨン急峻さが変わる。右上がりか右下がりか変わるたびにS字グラフもパッと反転し、線形グラフと同じく右上がりか右下がりになる
1. 線形グラフを上下に動かすと、グラフと横軸が交わる点が左右に動く。ここが 決定境界を表しているため、連動してs字グラフは左右に動く。

### 損失関数
データへの「フィット感」を測る指標。これが小さいほど良い。  
「パラメータ+損失値」空間内のグラフ

#### デファクトスタンダードの損失関数
* 線形回帰の場合： 平均二乗誤差
* ロジスティック回帰の場合： 交差エントロピー誤差

どちらのグラフも下に凸の放物面であり唯一の最小値を持つので、勾配降下法で安心して底を目指せる。  
※平均二乗誤差の場合は正規方程式で一撃だが、特徴量の数が非常に多い（数万を超えるような）場合は勾配降下法の方がコスパがいい場合がある。
※ロジスティック回帰の交差エントロピー誤差の関数は必ず下に凸になるが、その最小値（最下部）がユニークな一点に定まるとは限らず、平坦な領域（面や線）になる場合がある。そのため正則化で最下点を作る。

### 交差エントロピー誤差（対数損失）とは？

データ1件あたりの損失関数

$$ L_{CE}(p, y) = - [ y \log(p) + (1-y) \log(1-p) ] $$

*   $y=1$ のとき、第2項は消えて $- \log(p)$ となる。
*   $y=0$ のとき、第1項は消えて $- \log(1-p)$ となる。

つまり、予測値が正解からかけ離れるほど損失値は急激に跳ね上がる  
すべての訓練データ（m件）に対する全体の損失関数は、これらの平均を取ったもの。  

交差エントロピー誤差の損失関数Jをパラメータで偏微分すると、シグモイド関数の微分の性質によって多くの項が打ち消し合い、結果的に線形回帰とそっくりな勾配の式が現れる。

### 類似性
ラッキーなことに、線形回帰とロジスティック回帰の勾配の統一式は、構造がほとんど同じ。  
両方とも、(予測値－正解ラベル)×特徴量、という項を全データについて平均している。  
唯一の違いは「予測値」の計算方法。  



### 勾配降下法
損失曲面の現在地から、その真下にあるパラメータ空間上で、「最も急な上り坂の方向」と「傾きの大きさ」を表す「勾配ベクトル」を計算する。  
この勾配ベクトルは上り坂の方向を指しているので、その真逆の方向に進めば、損失曲面の坂を下ることができる。つまり進むべき移動方向は「勾配ベクトル × (-1)」。  
移動距離（パラメータの更新量）は、勾配ベクトルの「傾きの大きさ」を元に決める。「傾き」の情報と「移動距離」の情報は本来別物だが、「学習率」という係数を掛けて調整(翻訳)し、そのまま現在のパラメータに足し合わせてしまう。  
少々トリッキーなアプローチだが、このプロセスを何度も繰り返して少しずつ損失関数の谷底（最適解）に近づいていくのが勾配降下法だ。

#### パラメータ更新の式
$$
\boldsymbol{\theta} := \boldsymbol{\theta} - \alpha \nabla J(\boldsymbol{\theta})
$$
※わかりやすく
![](./img/gradient-descent.png)



### L1L2正則化
実務においてはほぼ必須の標準装備

バイアス項（切片 θ0）は正則化の対象から外すのが一般的。これは、切片はモデル全体の底上げを調整する役割であり、その大きさを罰するべきではない、という考え方に基づいている。
もし、より実践的な形にするなら、ペナルティ計算を alpha * (np.abs(T1)) や alpha * (T1**2) のように重み θ1 のみに適用します。


新しい曲面には、元のピラミッドが持っていた鋭い「折れ目」が残ります。
最小値: この新しい曲面で最も低い点（最小値）を探すと、その滑らかな底の部分ではなく、折れ目の谷底のどこかに落ち着く可能性が高くなります。そして、その折れ目上では、いずれかのパラメータが0になっています。
結論：「折れ目が強く深くなる」
そして、「正則化が強いほど折れ目が強く深くなる」というご意見も正しいです。
正則化項にかかる係数 
λ
λ
 を大きくすることは、幾何学的には「ひし形を小さくする」こと、あるいは曲面で言えば「逆ピラミッドをより急峻（steep）にする」ことに相当します。




機械学習の線形モデルに関する質問です
L1正則化ではいくつかのパラメータが0になりやすいですが、その理屈は以下のイメージで正しいですか？

わかりやすく、パラメータ二つ(θ0、θ1)で決まる損失曲面グラフを考える
L1正則化とはつまり、原点を中心とした逆ピラミッドグラフで、損失曲面グラフを押し上げる操作である
この時、損失曲面グラフには逆ピラミッドの稜線に沿って軽い折り目のような谷ができる。※L1正則化が強いほど谷は深くなる
グラフの最小値(の近似)を求めるとき、この谷底のどこかになる可能性が高い。※その可能性はL1正則化が強いほど高くなる。谷が深いから。
そしてその谷底のどこかとは、θ0かθ1軸上になるため、特定のパラメータが0になる


超簡単に表現すれば、なだらかな局面に折れ目のような溝があると、小さい球を転がしたときにその溝に「スポッツ」とはまりやすくなるイメージですか？溝が深いほどはまりやすくなる





はい、あなたの理解はほぼ正しいです！L1正則化（Lasso正則化）がスパースな解（一部のパラメータが0になる解）をもたらすメカニズムについて、直感的でわかりやすい説明になっています。以下に補足と修正を加えながら、正確な理解を確認しましょう。

あなたの説明の要点
逆ピラミッドのイメージ：L1正則化項（逆ピラミッド）が損失関数に加わることで、損失曲面に「谷」ができる。

谷の性質：谷は逆ピラミッドの稜線（θ0軸やθ1軸）に沿って形成され、正則化が強いほど谷は深くなる。

最適解の位置：最適解は谷底のどこか（特に軸上）になりやすく、結果として特定のパラメータが0になる。

正しいポイント
幾何学的な解釈：L1正則化項は確かに原点を中心とした「ダイヤモンド形」（逆ピラミッド）の等高線を持ち、損失曲面に「角」を作ります。この角が軸上（パラメータが0になる点）に最適解を誘導します。

スパース性の起源：最適解が角（稜線）に位置しやすいため、一部のパラメータが0になります。

補足・修正すべき点
「谷」の表現：

L1正則化は「谷」というより、「角（かど）」や「稜線」を作ると表現する方が正確です。損失曲面全体が逆ピラミッドで「押し上げられる」ことで、元の損失関数の最小値付近に角が生じ、その角が軸上に位置します。

「谷」という表現は、L2正則化（リッジ回帰）の円形の等高線と混同される可能性があります。L2ではなだらかな谷ができますが、L1では尖った角ができます。）

最適解が軸上になる理由：

数学的には、L1正則化項の微分が不連続（原点で未定義）なため、最適解が原点と接する（パラメータが0になる）ことが起こります。

直感的には、逆ピラミッドの「角」は軸上にあり、損失関数と正則化項のバランスを取ると、角が最適解になりやすいからです。

「近似」のニュアンス：

最適解は必ずしも厳密に軸上になるわけではありませんが、正則化が強いほど高い確率で軸上（スパースな解）になります。







「なだらかな斜面に鋭い溝（折れ目）があり、小さい球が溝にスポッとはまりやすくなる」 という表現は、L1正則化の振る舞いを非常に直感的に捉えています。












3. ご自身のイメージとの対応
「原点を中心とした逆ピラミッドグラフで、損失曲面グラフを押し上げる操作」: これは「損失関数＋L1正則化項」という新しい損失関数を考えることに対応します。L1正則化項の形は、原点で尖った逆ピラミッド（4角錐）のような形をしているため、このイメージは非常に的確です。[12]
「逆ピラミッドの稜線に沿って軽い折り目のような谷ができる」: 正しいです。新しい損失関数は、元の滑らかなお椀状のグラフに、軸上で尖った谷（微分不可能な点）を持つ形になります。[12]
「グラフの最小値は、この谷底のどこかになる可能性が高い」: その通りです。最適化の過程で、解はこの鋭い谷に「滑り落ち」やすくなります。
「谷底のどこかとはつまり、θ0軸上かθ1軸上であるため、特定のパラメータが0になる」: これが結論です。谷底が軸上にあるため、その点の座標、つまりパラメータのどちらかが0になります。













<a id="決定木"></a>
## 決定木


### モデルの用途
* 線形回帰：回帰  
* ロジスティック回帰：分類  



真偽値を返す条件による枝分かれで構成された木構造を学習する






















<a id="ランダムフォレスト"></a>
## ランダムフォレスト

* 沢山の小さな決定木をつくり組み合わせる  
* データと特徴量をランダムにサンプリングし、多様な木を作る
* 多数決や平均をとることで、個々の木の予測の不安定さを打ち消し、安定した予測を得る。  







はい、おっしゃる通りです。ご提示いただいた3つは、ランダムフォレストを構築する上で最も重要で、一般的に調整される主要なハイパーパラメータです。

それぞれのパラメータについて、少し詳しく解説します。（多くのライブラリで使われるscikit-learnでの引数名も併記します）

主要なハイパーパラメータ
1. 構築する決定木の数 (n_estimators)

内容: アンサンブル学習に含める決定木の総数です。

影響:

増やす: モデルの性能が安定し、過学習（オーバーフィッティング）しにくくなります。予測結果のばらつきが小さくなります。

減らす: 計算時間が短くなりますが、性能が不安定になったり、過学習しやすくなる可能性があります。

傾向: 一般的に、この値は大きいほど良いとされますが、ある一定数を超えると性能向上は頭打ちになり、計算時間だけが増えていきます。まずは100程度から始め、必要に応じて増やすのが一般的です。

2. 使用する特徴量の数 (max_features)

内容: 各決定木が分岐（ノードを分割）する際に、候補としてランダムに選ぶ特徴量の最大数です。

影響:

小さくする: 各決定木が使う特徴量がバラバラになり、木々の多様性が増します。これにより、モデル全体の汎化性能が向上する傾向があります。これがランダムフォレストの「ランダム」たる所以の一つです。

大きくする (全特徴量数に近づける): 各決定木が似通ったものになり、モデル全体の性能が低下する可能性があります（特に特徴量間に相関が強い場合）。

一般的な設定値:

分類問題: sqrt(全特徴量数) （全特徴量数の平方根）

回帰問題: 全特徴量数 / 3

3. 木の最大の深さ (max_depth)

内容: 各決定木の最大の深さ（階層の数）を制限します。

影響:

深くする (値を大きくする): 個々の決定木は訓練データに対してより複雑なモデルを作ろうとし、過学習しやすくなります。

浅くする (値を小さくする): 個々の決定木は単純なモデルになり、過学習を防ぐ効果があります。ただし、浅すぎると表現力が不足し、学習不足（アンダーフィッティング）になる可能性があります。

傾向: デフォルト（制限なし）だと、ノードが純粋になるまで木が成長するため、過学習のリスクがあります。この値を適切に設定することで、モデルの複雑さをコントロールできます。

その他の重要なパラメータ

上記3つに加えて、以下のパラメータも性能に影響を与えることがあります。

min_samples_split: ノードを分割するために必要な最小サンプル数。これを大きくすると、より一般的な（過学習しにくい）モデルになります。

min_samples_leaf: 葉ノード（末端のノード）に存在しなければならない最小サンプル数。これも過学習を防ぐために使われます。

bootstrap: データをサンプリングする際に、復元抽出（同じサンプルを複数回選ぶことを許可する）を行うかどうか。Trueがデフォルトで、これがランダムフォレストの「バギング」という手法の根幹をなします。

まとめ

ご質問いただいた**「決定木の数」「使用する特徴量の数」「木の最大の深さ」**の3つが、ランダムフォレストの性能を左右する最も重要なパラメータです。

実際にモデルを構築する際は、これらのパラメータをグリッドサーチやランダムサーチといった手法と**交差検証（クロスバリデーション）**を組み合わせて、データに最適な値を見つけ出すのが一般的です。

















はい、その理解で全く問題ありません。非常に的確な洞察だと思います！

おっしゃる通り、ランダムフォレストと勾配ブースティング決定木は、同じ「決定木」を使いながらも、そのフィロソフィーやキャラクター性が全く異なります。

ランダムフォレストのキャラクター：「民主的な専門家集団」

ランダムフォレストは決定木の純粋な発展形というか、決定木を工夫してより有効活用できるようにしたアルゴリズムって感じですかね？

まさにその通りです。

主役は「決定木」: あくまで決定木が主役です。

決定木の弱点を克服: 単一の決定木は、データを完璧に分類しようとしすぎて過学習（訓練データに適合しすぎて、未知のデータに対応できない状態）しやすい、という弱点があります。

工夫（民主主義）:

多様な意見を集める: 少しずつ違うデータ（ブートストラップサンプリング）と、少しずつ違う視点（特徴量のランダム選択）を使って、たくさんの「個性的だけど、少し未熟な決定木」を並列に作ります。

多数決で決める: それらの決定木たちに一斉に予測させ、その結果の多数決（分類）や平均（回帰）で最終的な答えを出します。

一人の天才（完璧な決定木）に頼るのではなく、たくさんの凡人（未熟な決定木）の意見を集約して、より堅牢で安定した結論を導く、というアプローチです。そのため**「決定木の純粋な発展形」**という表現は非常にしっくりきます。

勾配ブースティング決定木のキャラクター：「師弟関係で成長する職人チーム」

勾配ブースティング決定木は発展的なテクニックの中の一つの道具として決定木を使う感じですか？

こちらも、まさにその通りです。

主役は「ブースティング」: 「勾配ブースティング」という「間違いを次に活かして、モデルをどんどん賢くしていく」という考え方が主役です。

決定木は「便利な道具」: そのブースティングを実現するための「弱い学習器（Weak Learner）」として、非力だけど扱いやすい**「浅い決定木」が非常に相性が良い**ため、道具として使われています。

工夫（師弟関係）:

最初の木（弟子）が予測: まず、簡単な決定木が予測を試みます。当然、間違いがたくさんあります。

次の木が「間違い」を学習: 2本目の木は、元々のデータではなく、1本目の木が間違えた部分（残差）を重点的に学習します。つまり、師匠が弟子の間違いを正すようなものです。

チームで協力: 3本目の木は、1本目と2本目を合わせたチーム全体の間違いを学習...というように、直列的に木を追加していき、モデル全体の精度を徐々に高めていきます。

前のモデルの間違いを次のモデルが修正していく、という逐次的なアプローチです。一つ一つの木は非力ですが、チーム全体で協力して一つの完璧な答えに近づけていきます。そのため**「発展的なテクニックの道具として決定木を使う」**という表現がぴったりです。

まとめ：キャラクター性の違い
項目	ランダムフォレスト (RF)	勾配ブースティング決定木 (GBDT)
関係性	並列・独立	直列・依存
木々の役割	それぞれが独立した専門家	前の木の間違いを修正する役割
学習方法	一斉に学習（並列計算が得意）	順番に学習（逐次的な計算）
目的	過学習の抑制・安定化	精度の最大化
比喩	民主主義・多数決	師弟関係・改善の積み重ね
得意なこと	安定した性能を出しやすい	チューニングすれば非常に高い精度が出る
苦手なこと	最高の精度を出すのは難しいことも	過学習しやすく、チューニングが繊細

このようにキャラクターが全く違うので、どちらが良いかはデータや目的に応じて使い分けることになります。素晴らしい着眼点でした！






















<a id="勾配ブースティング"></a>
## 勾配ブースティング

* 沢山の小さな決定木をつくり組み合わせる  
* 新しいモデルは、前のモデルの予測と実際の値との残差(損失関数の勾配)を予測するように学習  
* 各モデルの結果を足し合わせる  

[ゼロから始める勾配ブースティング決定木の理論](https://zenn.dev/dalab/articles/9c843f0ec8aabf)









