# テーブルデータを用いた教師あり学習のアルゴリズム

**重要：万能なアルゴリズムはない！**


* [線形モデル](#線形モデル)
    * 線形回帰
    * ロジスティック回帰
    * ※正則化
        * L1正則化
        * L2正則化
* 決定木ベースモデル
    * 決定木
    * [ランダムフォレスト](#ランダムフォレスト)
    * [勾配ブースティング三兄弟](#勾配ブースティング)
        * XGBoost：落ち着いてる長男
        * LightGBM：スピードスターの次男
        * CatBoost：個性的な三男
* 距離ベースモデル
    * k-NN
* 単純ベイズモデル
    * Naive Bayes



<a id="線形モデル"></a>
## 線形モデル

### 前提
データに最もフィットする重み（w）とバイアス（b）を見つけるのが学習。   
**バイアスも「常に入力値が1の特徴量の重み」と捉えることでわかりやすくなる。**

### 基本
| | **線形回帰** | **ロジスティック回帰** |
| :--- | :--- | :--- |
| **モデル** | $h(\mathbf{x}') = \mathbf{w}'^T \mathbf{x}'$ | $p(\mathbf{x}') = \sigma(\mathbf{w}'^T \mathbf{x}')$ |
| **損失関数** | **平均二乗誤差**<br>$J = \frac{1}{2m} \sum (h - y)^2$ | **交差エントロピー誤差**<br>$J = - \frac{1}{m} \sum [y \log(p) + (1-y)\log(1-p)]$ |
| **勾配** | $\frac{\partial J}{\partial w'_j} = \frac{1}{m} \sum (h - y) x'_j$ | $\frac{\partial J}{\partial w'_j} = \frac{1}{m} \sum (p - y) x'_j$ |



### モデルの用途
* 線形回帰：回帰  
* ロジスティック回帰：分類  
※ロジスティック回帰は、線形回帰の出力をシグモイド関数に通して変換したもの。回帰の結果を確率と捉え、分類に利用する。  

### モデルの幾何学的イメージ
* 線形回帰：直線をくるくる傾けながら上下に移動させてデータにフィットさせる
* ロジスティック回帰：S字カーブの急峻さをビヨンビヨン変えたり反転させながら左右に移動させてデータにフィットさせる

#### ロジスティック回帰のモデルについて
* 内部の線形グラフ（横軸：特徴量x, 縦軸：線形スコアz）※シグモイド関数に通す前の、線形回帰と同じ部分のグラフ。z = wx + b で表されるただの直線。
* 最終的なS字カーブ（横軸：特徴量x, 縦軸：確率p）

#### 内部の線形グラフのパラメーター操作による、S字グラフ連動の仕方のイメージ
1. 内部の線形グラフをくるくる回すと、S字グラフも連動してビヨンビヨン急峻さが変わる。右上がりか右下がりか変わるたびにS字グラフもパッと反転し、線形グラフと同じく右上がりか右下がりになる
1. 線形グラフを上下に動かすと、グラフと横軸が交わる点が左右に動く。ここが 決定境界を表しているため、連動してs字グラフは左右に動く。

### 損失関数
データへの「フィット感」を測る指標。これが小さいほど良い。  
「パラメータ+損失値」空間内のグラフ

#### デファクトスタンダードの損失関数
* 線形回帰の場合： 平均二乗誤差
* ロジスティック回帰の場合： 交差エントロピー誤差

どちらのグラフも下に凸の放物面であり唯一の最小値を持つので、勾配降下法で安心して底を目指せる。  
※平均二乗誤差の場合は正規方程式で一撃だが、特徴量の数が非常に多い（数万を超えるような）場合は勾配降下法の方がコスパがいい場合がある。


### 交差エントロピー誤差（対数損失）とは？

データ1件あたりの損失関数

$$ L_{CE}(p, y) = - [ y \log(p) + (1-y) \log(1-p) ] $$

*   $y=1$ のとき、第2項は消えて $- \log(p)$ となる。
*   $y=0$ のとき、第1項は消えて $- \log(1-p)$ となる。

つまり、予測値が正解からかけ離れるほど損失値は急激に跳ね上がる

すべての訓練データ（$m$件）に対する全体の損失関数は、これらの平均を取ったもの。  

$$ J(\boldsymbol{w}, b) = - \frac{1}{m} \sum_{i=1}^{m} [ y_i \log(p_i) + (1-y_i) \log(1-p_i) ] $$
※$p_i = \sigma(\boldsymbol{w}^T \boldsymbol{x}_i + b)$

交差エントロピー誤差の損失関数$J$をパラメータで偏微分すると、シグモイド関数の微分の性質（$\sigma'(z) = \sigma(z)(1-\sigma(z))$）によって多くの項が打ち消し合い、結果的に線形回帰とそっくりな勾配の式が現れる。



### 類似性
ラッキーなことに、線形回帰とロジスティック回帰の勾配の統一式は、構造がほとんど同じ。

*   **線形回帰の勾配:**
    $$ \frac{\partial J}{\partial w'_j} = \frac{1}{m} \sum_{i=1}^{m} (\underbrace{h(\mathbf{x}'^{(i)})}_{\text{予測値}} - y^{(i)}) x'^{(i)}_j $$

*   **ロジスティック回帰の勾配:**
    $$ \frac{\partial J}{\partial w'_j} = \frac{1}{m} \sum_{i=1}^{m} (\underbrace{p^{(i)}}_{\text{予測値}} - y^{(i)}) x'^{(i)}_j $$

両方とも、
$$ (\text{予測値} - \text{正解ラベル}) \times \text{特徴量} $$
という形の項を、全データについて平均している。

唯一の違いは「予測値」の計算方法。
*   線形回帰: 予測値は線形結合そのもの $h(\mathbf{x}') = \mathbf{w}'^T \mathbf{x}'$
*   ロジスティック回帰: 予測値は線形結合をシグモイド関数に入れたもの $p = \sigma(\mathbf{w}'^T \mathbf{x}')$




---------------
以下修正版




$$ J(\mathbf{w}, b) = - \frac{1}{m} \sum_{i=1}^{m} [ y_i \log(p_i) + (1-y_i) \log(1-p_i) ] $$

ただし、$p_i = \sigma(\mathbf{w}^T \mathbf{x}_i + b)$ です。

交差エントロピー誤差の損失関数$J$をパラメータで偏微分すると、シグモイド関数の微分の性質（$\sigma'(z) = \sigma(z)(1-\sigma(z))$）によって多くの項が打ち消し合い、結果的に線形回帰とそっくりな勾配の式が現れる。

### 類似性
ラッキーなことに、線形回帰とロジスティック回帰の勾配の統一式は、構造がほとんど同じ。

*   **線形回帰の勾配:**
    $$ \frac{\partial J}{\partial w'_j} = \frac{1}{m} \sum_{i=1}^{m} (h(\mathbf{x}'^{(i)}) - y^{(i)}) x'^{(i)}_j $$

*   **ロジスティック回帰の勾配:**
    $$ \frac{\partial J}{\partial w'_j} = \frac{1}{m} \sum_{i=1}^{m} (p^{(i)} - y^{(i)}) x'^{(i)}_j $$

両方とも、

**（予測値 - 正解ラベル） × 特徴量**

という形の項を、全データについて平均している。

唯一の違いは「予測値」の計算方法。
*   線形回帰: 予測値は線形結合そのもの $h(\mathbf{x}') = \mathbf{w}'^T \mathbf{x}'$
*   ロジスティック回帰: 予測値は線形結合をシグモイド関数に入れたもの $p = \sigma(\mathbf{w}'^T \mathbf{x}')$



















### 勾配降下法








































勾配ベクトルの始点は、今自分が立っている山の斜面から垂直方向に真下にある底面の点
その点から「斜面がこっち方向に一番上ってますよ!」という情報を表す一本の矢印が伸びている

「今自分が立っている山の斜面」というのが、(n+1)次元空間上の点 (w1, w2, ..., wn, Loss) です。
「垂直方向に真下にある底面の点」というのが、n次元のパラメータ空間上の点 (w1, w2, ..., wn) です。
この点が、勾配ベクトルという矢印の**始点（出発点）**になります。まさにその通りです。



まとめると
あなたが今立っている場所（パラメータ）から、損失という「山」の最も低い場所（最適解）を探したいとき、
1. まず、自分が立っている山の斜面の真下、地図（パラメータ空間）上の現在地を確認します。（＝ベクトルの始点）
1. 次に、その場所で最も急な上り坂がどちらを向いているかを示す**コンパス（勾配ベクトル）**を見ます。
1. そして、そのコンパスが指し示す方向とは真逆の方向に一歩踏み出します。（＝勾配降下法）
この1〜3のプロセスを繰り返していくのが、勾配降下法による最適化です。


目的: 損失関数の最小化（山下り）
地図: パラメータ空間
地形: 損失関数のグラフ（山や谷）
道具: 微分（コンパス・傾斜計）
戦略: 勾配降下法（道具を使って、常に一番急な下り坂を進む）



パラメータ空間に、勾配ベクトルというガイド情報を投影する。
「損失関数から計算された勾配という『ガイド情報』を、パラメータ空間に『投影』し、その指示に従って現在のパラメータ（座標）を移動させる。その移動の勢いを調整するのが学習率である。」
?学習率 η は、その「ナビ情報」を、実際にFを動かすための**「具体的な修正量」に変換する**ための、非常に重要な”翻訳係”の役割も担っているのです。

勾配下降法について
以下の式は正しい？
newパラメータベクトル　= oldパラメータベクトル + 学習率 * (-勾配ベクトル)

学習率で「勾配」を「パラメータの更新量」に変換する
































<a id="ランダムフォレスト"></a>
## ランダムフォレスト

* 沢山の小さな決定木をつくり組み合わせる  
* データと特徴量をランダムにサンプリングし、多様な木を作る
* 多数決や平均をとることで、個々の木の予測の不安定さを打ち消し、安定した予測を得る。  


<a id="勾配ブースティング"></a>
## 勾配ブースティング

* 沢山の小さな決定木をつくり組み合わせる  
* 新しいモデルは、前のモデルの予測と実際の値との残差(損失関数の勾配)を予測するように学習  
* 各モデルの結果を足し合わせる  

[ゼロから始める勾配ブースティング決定木の理論](https://zenn.dev/dalab/articles/9c843f0ec8aabf)









